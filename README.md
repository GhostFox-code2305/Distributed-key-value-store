# Distributed Key-Value Store

A production-grade distributed database implementation in Go, featuring LSM trees, Raft consensus, and consistent hashing.

## Progress Overview

- [x] **Week 1**: Foundation & WAL âœ…
- [x] **Week 2**: LSM Tree (MemTable + SSTables) âœ… â† **I AM HERE**
- [ ] **Week 3**: Compaction & Optimization
- [ ] **Week 4**: Networking (gRPC)
- [ ] **Week 5**: Consistent Hashing
- [ ] **Week 6**: Replication
- [ ] **Week 7-9**: Raft Consensus
- [ ] **Week 10**: Production Polish

---

## Week 2: LSM Tree Implementation âœ…

### What I Built

1. **MemTable (Skip List)**: In-memory sorted data structure
   - O(log n) insert/search operations
   - 64MB size threshold before flushing
   - Thread-safe concurrent operations

2. **SSTable (Sorted String Table)**: Immutable on-disk files
   - Sorted key-value pairs for efficient lookups
   - Index block for fast key location (binary search)
   - Footer with metadata

3. **LSM Store**: Orchestrates MemTable and SSTables
   - Write path: WAL â†’ MemTable â†’ SSTable (when full)
   - Read path: MemTable â†’ SSTables (newest to oldest)
   - Automatic flushing when MemTable reaches 64MB

4. **Tombstones**: Proper deletion handling
   - Deletes write tombstone markers
   - Prevents deleted keys from reappearing

### Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Write Path                             â”‚
â”‚                                                              â”‚
â”‚  PUT(key, value)                                            â”‚
â”‚         â”‚                                                    â”‚
â”‚         â”œâ”€â”€â–º WAL (durability)                               â”‚
â”‚         â”‚                                                    â”‚
â”‚         â””â”€â”€â–º MemTable (Skip List, 64MB max)                 â”‚
â”‚                    â”‚                                         â”‚
â”‚                    â”‚ When full                               â”‚
â”‚                    â–¼                                         â”‚
â”‚              Flush to SSTable                                â”‚
â”‚              (immutable, sorted)                             â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Read Path                              â”‚
â”‚                                                              â”‚
â”‚  GET(key)                                                   â”‚
â”‚         â”‚                                                    â”‚
â”‚         â”œâ”€â”€â–º 1. Check MemTable â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Found? Return   â”‚
â”‚         â”‚                                                    â”‚
â”‚         â”œâ”€â”€â–º 2. Check Immutable MemTable â”€â–º Found? Return   â”‚
â”‚         â”‚                                                    â”‚
â”‚         â””â”€â”€â–º 3. Check SSTables (newest â†’ oldest)            â”‚
â”‚                    â”‚                                         â”‚
â”‚                    â””â”€â”€â–º Binary search in index              â”‚
â”‚                          Read from data block               â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Features

âœ… **Skip List MemTable**: O(log n) sorted data structure  
âœ… **Automatic Flushing**: 64MB threshold prevents memory overflow  
âœ… **SSTable Format**: Efficient on-disk storage with index  
âœ… **Crash Recovery**: WAL replay restores MemTable state  
âœ… **Tombstone Deletion**: Proper handling of deleted keys  
âœ… **Multi-level Reads**: MemTable â†’ SSTables hierarchy

### File Format

#### SSTable Structure
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Data Block                   â”‚
â”‚  [key1_len][key1][val1_len][val1]   â”‚
â”‚  [key2_len][key2][val2_len][val2]   â”‚
â”‚  ...                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚         Index Block                  â”‚
â”‚  [key1_len][key1][offset1]          â”‚
â”‚  [key2_len][key2][offset2]          â”‚
â”‚  ...                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚         Footer (16 bytes)            â”‚
â”‚  [index_offset: 8 bytes]            â”‚
â”‚  [num_entries: 4 bytes]             â”‚
â”‚  [magic_number: 4 bytes]            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Getting Started

### Prerequisites

- Go 1.21 or higher
- Windows/Linux/macOS

### Installation

```bash
# Clone or create the project
mkdir distributed-kv && cd distributed-kv

# Initialize Go module
go mod init kvstore

# Create directory structure
mkdir -p storage cmd/server data
```

### Running the Server

```bash
# Run from project root
go run cmd/server/main.go

# Output:
# ğŸš€ Distributed KV Store started (LSM Tree Mode)
# ğŸ“ Data directory: ./data
# ğŸ’¾ MemTable threshold: 64MB
# ğŸ“ Commands: PUT <key> <value>, GET <key>, DELETE <key>, STATS, QUIT
```

### Running Tests

```bash
# Run all tests
go test ./storage/...

# Run with verbose output
go test -v ./storage/...

# Run specific test
go test -v ./storage/ -run TestLSMStore_MemTableFlush

# Run benchmarks
go test -bench=. ./storage/...
```

### Example Usage

```bash
# Start the server
$ go run cmd/server/main.go

# Basic operations
> PUT user:1 {"name":"Alice","age":30}
âœ… OK

> GET user:1
ğŸ“¦ {"name":"Alice","age":30}

> DELETE user:1
ğŸ—‘ï¸  Deleted

> GET user:1
âŒ Error: key not found

# Check statistics
> STATS
ğŸ“Š Statistics:
  memtable_size: 1024
  num_sstables: 0

# Write large amount of data to trigger flush
> PUT large_key_1 [1MB of data]
âœ… OK
... (repeat 65 times)

> STATS
ğŸ“Š Statistics:
  memtable_size: 512
  num_sstables: 1    # SSTable created!
```

### Testing SSTable Flush

```bash
# This script writes enough data to trigger flush
$ go run cmd/server/main.go

> PUT test_1 [paste 1KB of text]
> PUT test_2 [paste 1KB of text]
... (repeat until you see flush happen)

> STATS
ğŸ“Š Statistics:
  memtable_size: 2048
  num_sstables: 1    # Data flushed to disk!
```

---

## Performance Characteristics

### Week 2 (LSM Tree)

**Write Performance:**
- **MemTable writes**: ~100-200ns (in-memory skip list)
- **With WAL**: ~1-2ms (includes fsync)
- **Throughput**: ~500-1000 writes/sec (WAL-limited)

**Read Performance:**
- **MemTable hits**: ~200ns (skip list lookup)
- **SSTable reads**: ~1-5ms (disk I/O + binary search)
- **Throughput**: 
  - Hot data (in MemTable): ~1M ops/sec
  - Cold data (SSTables): ~200-1000 ops/sec

**Space Efficiency:**
- **Compression**: None (Week 3 feature)
- **Overhead**: ~16 bytes per key-value pair (index + metadata)

**Comparison with Week 1:**

| Metric | Week 1 (Simple Map) | Week 2 (LSM Tree) |
|--------|---------------------|-------------------|
| Max dataset size | RAM limited | Disk limited |
| Write throughput | ~500 ops/sec | ~500 ops/sec |
| Read latency (hot) | ~100ns | ~200ns |
| Read latency (cold) | N/A | ~1-5ms |
| Scalability | Poor | Good |

---

## Code Structure

```
distributed-kv/
â”œâ”€â”€ storage/
â”‚   â”œâ”€â”€ wal.go           # Write-Ahead Log (Week 1)
â”‚   â”œâ”€â”€ memtable.go      # Skip List MemTable (Week 2) âœ¨
â”‚   â”œâ”€â”€ sstable.go       # SSTable writer/reader (Week 2) âœ¨
â”‚   â”œâ”€â”€ lsm_store.go     # LSM orchestration (Week 2) âœ¨
â”‚   â””â”€â”€ lsm_store_test.go # Comprehensive tests (Week 2) âœ¨
â”œâ”€â”€ cmd/
â”‚   â””â”€â”€ server/
â”‚       â””â”€â”€ main.go      # Updated CLI (Week 2) âœ¨
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ wal.log         # Write-ahead log
â”‚   â”œâ”€â”€ sstable_0.db    # First SSTable (created on flush)
â”‚   â”œâ”€â”€ sstable_1.db    # Second SSTable
â”‚   â””â”€â”€ ...
â”œâ”€â”€ go.mod
â””â”€â”€ README.md
```

---

## Technical Deep Dive

### Why Skip List for MemTable?

**Advantages:**
- Probabilistic balancing (simpler than Red-Black trees)
- Lock-free read operations possible
- Good cache locality
- Average O(log n) complexity

**Trade-offs:**
- Slightly more memory than hash maps
- Worst-case O(n) (extremely rare)

### Why LSM Tree?

**Perfect for:**
- Write-heavy workloads (sequential writes are fast)
- Large datasets (don't need to fit in RAM)
- Range queries (sorted data)

**Not ideal for:**
- Read-heavy with random access (need to check multiple levels)
- Small datasets that fit in RAM (simpler structures work better)

### SSTable Design Decisions

**Why immutable?**
- Simplifies concurrent access (no locks needed)
- Enables efficient caching
- Makes compaction easier

**Why separate index block?**
- Fast key lookup without scanning entire file
- Binary search on small in-memory index
- Only one disk seek per GET operation

---

## What's Next: Week 3

In **Week 3**, we'll add:
- **Compaction**: Merge overlapping SSTables to reclaim space
- **Bloom Filters**: Skip SSTables that definitely don't have a key
- **Compression**: Reduce disk usage
- **Better stats**: Track read/write amplification

These optimizations will make the database production-ready!

---

## Technical Decisions & Trade-offs

### MemTable Size (64MB)

**Why 64MB?**
- Large enough: Amortizes flush cost
- Small enough: Fits comfortably in RAM
- Industry standard: RocksDB uses 64-256MB

**Trade-offs:**
- Larger = fewer flushes, more memory
- Smaller = faster recovery, less memory

### Tombstones

**Why not delete immediately?**
- SSTables are immutable
- Key might exist in older SSTables
- Compaction will eventually remove tombstones

---

## Common Issues & Solutions

### Issue: "MemTable never flushes"

**Solution**: You need to write >64MB of data. Try this test:

```go
for i := 0; i < 70000; i++ {
    key := fmt.Sprintf("key_%d", i)
    value := make([]byte, 1024) // 1KB
    store.Put(key, value)
}
```

### Issue: "Reads are slow after flush"

**Expected behavior**: Reads from disk (SSTables) are 1000x slower than RAM (MemTable). Week 3 will add Bloom filters to help.

### Issue: "Disk usage grows quickly"

**Expected behavior**: No compaction yet. Each flush creates a new SSTable. Week 3 adds compaction to merge and reclaim space.

---

## Resources

- [Log-Structured Merge-Trees (Original Paper)](https://www.cs.umb.edu/~poneil/lsmtree.pdf)
- [Skip Lists: A Probabilistic Alternative to Balanced Trees](https://15721.courses.cs.cmu.edu/spring2018/papers/08-oltpindexes1/pugh-skiplists-cacm1990.pdf)
- [RocksDB Wiki](https://github.com/facebook/rocksdb/wiki)
- [Designing Data-Intensive Applications](https://dataintensive.net/)

---

## Testing Checklist

Week 2 tests:
- [x] Basic Put/Get/Delete operations
- [x] MemTable automatic flush (>64MB)
- [x] Reading from SSTables
- [x] Crash recovery with WAL
- [x] Skip list maintains sorted order
- [x] Tombstone deletion

Run all tests:
```bash
go test -v ./storage/...
```

---

**Status**: Week 2 Complete âœ…  
**Next**: Week 3 - Compaction & Optimization  
**Lines of Code**: ~800 (total: ~1600)
