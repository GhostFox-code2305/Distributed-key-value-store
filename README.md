# Distributed Key-Value Store

A production-grade distributed database implementation in Go, featuring LSM trees, Bloom filters, compaction, Raft consensus, and consistent hashing. This project demonstrates advanced systems programming concepts used in modern databases like Cassandra, RocksDB, and DynamoDB.

---

## üéØ Project Overview

This is a multi-week journey building a distributed key-value database from scratch. Each week adds critical features that production databases need:

- **Week 1**: Write-Ahead Log (WAL) & crash recovery
- **Week 2**: LSM Tree with MemTable & SSTables
- **Week 3**: Bloom Filters & Compaction 
- **Week 4**: Networking (gRPC) ‚úÖ ‚Üê **I AM HERE**
- **Week 5**: Consistent Hashing
- **Week 6**: Replication
- **Week 7-9**: Raft Consensus
- **Week 10**: Production Polish

---

## üìä Architecture Overview
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         CLIENT                                  ‚îÇ
‚îÇ                            ‚îÇ                                     ‚îÇ
‚îÇ                            ‚ñº                                     ‚îÇ
‚îÇ                   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                           ‚îÇ
‚îÇ                   ‚îÇ   CLI Interface  ‚îÇ                           ‚îÇ
‚îÇ                   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                           ‚îÇ
‚îÇ                            ‚îÇ                                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      LSM STORE                                  ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îÇ  Write Path:                                                    ‚îÇ
‚îÇ  PUT ‚Üí WAL ‚Üí MemTable (64MB) ‚Üí SSTable (when full)             ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îÇ  Read Path:                                                     ‚îÇ
‚îÇ  GET ‚Üí MemTable ‚Üí Bloom Filter Check ‚Üí SSTables                ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îÇ  Background:                                                    ‚îÇ
‚îÇ  Compaction Manager (runs every 30s)                           ‚îÇ
‚îÇ    - Merges SSTables                                            ‚îÇ
‚îÇ    - Removes tombstones                                         ‚îÇ
‚îÇ    - Reclaims disk space                                        ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        DISK                                     ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ wal.log           (Write-Ahead Log)                       ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ sstable_0.db      (Sorted String Table)                   ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ sstable_1.db      (Sorted String Table)                   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ ...                                                        ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üöÄ Quick Start

### Prerequisites

- Go 1.21 or higher
- Windows/Linux/macOS

### Installation
```bash
# Clone or create the project
mkdir distributed-kv && cd distributed-kv

# Initialize Go module
go mod init kvstore

# Create directory structure
mkdir -p storage cmd/server data
```

### Running the Server
```bash
# Start the database
go run cmd/server/main.go

# Or specify custom data directory
go run cmd/server/main.go -data ./my-data
```

### Running Tests
```bash
# Run all tests
go test -v ./storage/

# Run specific test suite
go test -v ./storage/ -run TestLSMStore
go test -v ./storage/ -run TestBloomFilter
go test -v ./storage/ -run TestCompaction

# Run benchmarks
go test -bench=. ./storage/
```

---

## üìù Usage Examples

### Basic Operations
```bash
> PUT user:1 {"name":"Alice","age":30}
‚úÖ OK

> GET user:1
üì¶ {"name":"Alice","age":30}

> DELETE user:1
üóëÔ∏è  Deleted

> GET user:1
‚ùå Error: key not found
```

### Check Statistics
```bash
> STATS

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë                    üìä STATISTICS                          ‚ïë
‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
‚ïë  üíæ Storage:                                              ‚ïë
‚ïë     MemTable Size:        2048       bytes              ‚ïë
‚ïë     Number of SSTables:   2                             ‚ïë
‚ïë                                                           ‚ïë
‚ïë  üå∏ Bloom Filter:                                         ‚ïë
‚ïë     Hits (skipped reads): 15                            ‚ïë
‚ïë     Misses (disk reads):  3                             ‚ïë
‚ïë     Hit Rate:             83.3%                         ‚ïë
‚ïë                                                           ‚ïë
‚ïë  üîÑ Compaction:                                           ‚ïë
‚ïë     Total Compactions:    1                             ‚ïë
‚ïë     Keys Removed:         500                           ‚ïë
‚ïë     Bytes Reclaimed:      2048576    bytes              ‚ïë
‚ïë     Last Compaction:      2024-12-22T15:45:12Z          ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
```

### Force Manual Compaction
```bash
> COMPACT
üîÑ Triggering manual compaction...
üîÑ Starting compaction (3 SSTables)
üìä Compaction stats: 1200 keys removed, 5242880 bytes reclaimed
‚úÖ Compaction completed in 1.23s
```

---

## üèóÔ∏è Project Structure
```
distributed-kv/
‚îú‚îÄ‚îÄ storage/
‚îÇ   ‚îú‚îÄ‚îÄ wal.go              # Write-Ahead Log (Week 1)
‚îÇ   ‚îú‚îÄ‚îÄ memtable.go         # Skip List MemTable (Week 2)
‚îÇ   ‚îú‚îÄ‚îÄ sstable.go          # SSTable writer/reader (Week 2 + Week 3)
‚îÇ   ‚îú‚îÄ‚îÄ lsm_store.go        # LSM orchestration (Week 2 + Week 3)
‚îÇ   ‚îú‚îÄ‚îÄ bloom_filter.go     # Bloom filter (Week 3)
‚îÇ   ‚îú‚îÄ‚îÄ compaction.go       # Compaction manager (Week 3)
‚îÇ   ‚îú‚îÄ‚îÄ lsm_store_test.go   # LSM tests
‚îÇ   ‚îú‚îÄ‚îÄ bloom_filter_test.go # Bloom filter tests
‚îÇ   ‚îî‚îÄ‚îÄ compaction_test.go  # Compaction tests
‚îú‚îÄ‚îÄ cmd/
‚îÇ   ‚îî‚îÄ‚îÄ server/
‚îÇ       ‚îî‚îÄ‚îÄ main.go         # CLI server
‚îú‚îÄ‚îÄ data/                   # Generated data files
‚îÇ   ‚îú‚îÄ‚îÄ wal.log
‚îÇ   ‚îú‚îÄ‚îÄ sstable_0.db
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ go.mod
‚îî‚îÄ‚îÄ README.md
```

---

## üìö Week-by-Week Progress

### Week 1: Foundation & Write-Ahead Log ‚úÖ

**What We Built:**
- Write-Ahead Log (WAL) for durability
- Simple in-memory key-value store
- Crash recovery by replaying WAL
- Basic Put/Get/Delete operations

**Key Concepts:**
- Durability through fsync
- Binary protocol design
- Crash recovery

**Performance:**
- Write throughput: ~500-1000 ops/sec (fsync-limited)
- Read throughput: ~1M+ ops/sec (memory-speed)

---

### Week 2: LSM Tree Implementation ‚úÖ

**What We Built:**
- **MemTable**: Skip List for sorted in-memory storage
- **SSTables**: Immutable sorted files on disk
- **Automatic Flushing**: 64MB threshold triggers disk write
- **Multi-level Reads**: MemTable ‚Üí SSTables hierarchy
- **Tombstones**: Proper deletion handling

**Key Concepts:**
- Log-Structured Merge Trees
- Skip List data structure (O(log n) operations)
- Immutable data structures
- Write amplification vs read amplification

**Performance:**
- Write latency: ~1-2ms (WAL + MemTable)
- Read latency (hot): ~200ns (MemTable)
- Read latency (cold): ~1-5ms (SSTables)
- Max dataset size: Disk-limited (not RAM-limited anymore!)

**File Format (SSTable):**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         Data Block                   ‚îÇ
‚îÇ  [key1_len][key1][val1_len][val1]   ‚îÇ
‚îÇ  [key2_len][key2][val2_len][val2]   ‚îÇ
‚îÇ  ...                                 ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ         Index Block                  ‚îÇ
‚îÇ  [key1_len][key1][offset1]          ‚îÇ
‚îÇ  [key2_len][key2][offset2]          ‚îÇ
‚îÇ  ...                                 ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ         Footer (16 bytes)            ‚îÇ
‚îÇ  [index_offset: 8 bytes]            ‚îÇ
‚îÇ  [num_entries: 4 bytes]             ‚îÇ
‚îÇ  [magic_number: 4 bytes]            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

### Week 3: Bloom Filters & Compaction ‚úÖ

**What We Built:**
- **Bloom Filters**: Probabilistic data structure to skip unnecessary SSTable reads
- **Compaction Manager**: Background process that merges SSTables
- **Space Reclamation**: Remove tombstones and duplicates
- **Enhanced Stats**: Track bloom filter effectiveness and compaction metrics

**Key Concepts:**
- Bloom filters (false positives possible, false negatives impossible)
- Size-tiered compaction strategy
- Read amplification reduction
- Space amplification reduction

**Performance Improvements:**
- **Read speedup**: 10-100x for keys that don't exist (bloom filter skips SSTables)
- **Disk usage**: 50-90% reduction after compaction
- **Write throughput**: Unchanged (~500-1000 ops/sec)

**Bloom Filter:**
- ~10 bits per key for 1% false positive rate
- For 1M keys: ~1.2MB bloom filter
- Fast lookups: O(k) where k = number of hash functions (~7)

**Updated SSTable Format:**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         Data Block                   ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ         Index Block                  ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ         Bloom Filter Block ‚ú®        ‚îÇ
‚îÇ  [serialized bloom filter]          ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ         Footer (28 bytes)            ‚îÇ
‚îÇ  [index_offset: 8 bytes]            ‚îÇ
‚îÇ  [bloom_offset: 8 bytes] ‚ú®         ‚îÇ
‚îÇ  [bloom_len: 4 bytes] ‚ú®            ‚îÇ
‚îÇ  [num_entries: 4 bytes]             ‚îÇ
‚îÇ  [magic_number: 4 bytes]            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Compaction Process:**
1. Triggers when >4 SSTables exist
2. Merges all SSTables into one
3. Removes tombstones (deleted keys)
4. Removes duplicate keys (keeps newest version)
5. Builds new bloom filter
6. Deletes old SSTables

---

## üéØ Key Features

### ‚úÖ Durability
- Write-Ahead Log ensures no data loss
- All writes are fsync'd to disk
- Automatic crash recovery

### ‚úÖ Performance
- **Writes**: Sequential writes to WAL and MemTable (fast!)
- **Reads**: 
  - Hot data (MemTable): ~200ns
  - Cold data (SSTables): ~1-5ms with bloom filter optimization
- **Scalability**: Dataset can exceed RAM size

### ‚úÖ Space Efficiency
- Compaction removes duplicates and tombstones
- Bloom filters are tiny (~10 bits per key)
- SSTable format is compact

### ‚úÖ Reliability
- WAL provides crash recovery
- Immutable SSTables prevent corruption
- Background compaction runs automatically

---

## üìä Performance Characteristics

### Current Performance (Week 3)

| Operation | Latency | Throughput | Notes |
|-----------|---------|------------|-------|
| PUT | ~1-2ms | ~500-1000 ops/sec | Limited by fsync |
| GET (hot) | ~200ns | ~1M+ ops/sec | MemTable hit |
| GET (cold, exists) | ~1-5ms | ~200-1000 ops/sec | SSTable read |
| GET (cold, not exists) | ~0.01ms | ~100K ops/sec | Bloom filter skip ‚ú® |
| DELETE | ~1-2ms | ~500-1000 ops/sec | Writes tombstone |
| Compaction | ~1-5s | N/A | Background process |

### Comparison: Week 1 vs Week 3

| Metric | Week 1 | Week 3 | Improvement |
|--------|--------|--------|-------------|
| Max dataset size | RAM limited (~16GB) | Disk limited (~1TB+) | 60x+ |
| Read latency (miss) | N/A | ~0.01ms | ‚àû (bloom filter) |
| Disk space efficiency | 100% | 50-10% | 2-10x (compaction) |
| Sorted data | ‚ùå No | ‚úÖ Yes | Enables range queries |
| Crash recovery | ‚úÖ Yes | ‚úÖ Yes | Same |

---

## üß™ Testing

### Run All Tests
```bash
go test -v ./storage/
```

### Test Coverage
- ‚úÖ Basic operations (PUT/GET/DELETE)
- ‚úÖ MemTable flush (>64MB)
- ‚úÖ SSTable read/write
- ‚úÖ Crash recovery
- ‚úÖ Bloom filter false positive rate
- ‚úÖ Compaction (merge + tombstone removal)
- ‚úÖ Concurrent access

### Benchmarks
```bash
go test -bench=. ./storage/

# Sample output:
# BenchmarkLSMStore_Put-8         1000    1234567 ns/op
# BenchmarkLSMStore_Get-8         10000    123456 ns/op
# BenchmarkBloomFilter_Add-8      50000000   25.3 ns/op
# BenchmarkBloomFilter_MayContain-8 100000000  10.1 ns/op
```

---

## üîß Configuration

### Tunable Parameters

**MemTable Size** (`storage/lsm_store.go`):
```go
const MemTableSizeThreshold = 64 * 1024 * 1024 // 64MB
```
- Larger = fewer flushes, more memory usage
- Smaller = more flushes, less memory usage

**Compaction Interval** (`storage/compaction.go`):
```go
compactionRate: 30 * time.Second // Run every 30 seconds
```
- More frequent = less disk usage, more CPU
- Less frequent = more disk usage, less CPU

**Compaction Trigger** (`storage/compaction.go`):
```go
if numSSTables <= 4 {
    return nil // Don't compact yet
}
```
- Higher threshold = more SSTables to check during reads
- Lower threshold = more frequent compactions

**Bloom Filter** (`storage/sstable.go`):
```go
estimatedKeys := 10000
falsePositiveRate := 0.01 // 1%
```
- Lower FPR = larger bloom filter, fewer false positives
- Higher FPR = smaller bloom filter, more false positives

---

## üéì Learning Outcomes

### Data Structures
- ‚úÖ Skip List (probabilistic balanced tree)
- ‚úÖ Bloom Filter (probabilistic set membership)
- ‚úÖ LSM Tree (write-optimized storage)
- ‚úÖ Write-Ahead Log (durability pattern)

### Systems Concepts
- ‚úÖ Crash recovery and durability
- ‚úÖ Binary protocols and serialization
- ‚úÖ Concurrent access with mutexes
- ‚úÖ Background goroutines
- ‚úÖ File I/O and fsync
- ‚úÖ Memory vs disk trade-offs

### Database Internals
- ‚úÖ Write amplification
- ‚úÖ Read amplification
- ‚úÖ Space amplification
- ‚úÖ Compaction strategies
- ‚úÖ Immutable data structures
- ‚úÖ Tombstones for deletes

---

## üêõ Troubleshooting

### Issue: Tests are slow
**Solution**: Tests that write 70MB+ of data take time. This is expected.

### Issue: Bloom filter false positives
**Solution**: This is normal! ~1% of negative lookups will incorrectly say "might exist". Adjust the false positive rate if needed.

### Issue: Disk usage grows quickly
**Solution**: Compaction runs every 30 seconds. Wait for it, or trigger manual compaction with `COMPACT` command.

### Issue: MemTable never flushes
**Solution**: You need to write >64MB of data. Use the test scripts provided.

### Issue: "cannot find package"
**Solution**: Run `go mod tidy` to update dependencies.

---

## üìñ References & Resources

### Papers
- [The Log-Structured Merge-Tree (LSM-Tree)](https://www.cs.umb.edu/~poneil/lsmtree.pdf) - Original LSM Tree paper
- [Bigtable: A Distributed Storage System](https://static.googleusercontent.com/media/research.google.com/en//archive/bigtable-osdi06.pdf) - Google's BigTable
- [Bloom Filters](https://en.wikipedia.org/wiki/Bloom_filter) - Wikipedia overview

### Books
- [Designing Data-Intensive Applications](https://dataintensive.net/) by Martin Kleppmann
- [Database Internals](https://www.databass.dev/) by Alex Petrov

### Similar Systems
- [RocksDB](https://rocksdb.org/) - Facebook's LSM-based storage engine
- [LevelDB](https://github.com/google/leveldb) - Google's key-value store
- [Cassandra](https://cassandra.apache.org/) - Distributed database using LSM trees
- [ScyllaDB](https://www.scylladb.com/) - High-performance Cassandra-compatible database

---

## üöÄ What's Next?

### Week 4: Networking (gRPC)
- Client-server architecture
- Remote procedure calls
- Protocol Buffers
- Multiple clients

### Week 5: Consistent Hashing
- Partition data across nodes
- Virtual nodes
- Rebalancing

### Week 6: Replication
- Primary-backup replication
- Quorum reads/writes
- Hinted handoff

### Week 7-9: Raft Consensus
- Leader election
- Log replication
- Strong consistency

### Week 10: Production Polish
- Monitoring (Prometheus metrics)
- Admin API
- Docker deployment
- Benchmarking suite

---

## ü§ù Contributing

This is a learning project! Feel free to:
- Experiment with the code
- Add new features
- Optimize performance
- Report issues
- Share improvements

---

## üìù License

This project is for educational purposes.

---

## üéâ Acknowledgments

Built following the architecture of production databases like:
- Apache Cassandra
- Google Bigtable
- Facebook RocksDB
- Amazon DynamoDB

Special thanks to the database systems community for sharing knowledge through papers, blogs, and open source implementations.

---

**Status**: Week 4 - Networking (gRPC)Complete ‚úÖ  
**Total Lines of Code**: ~2,500  
**Test Coverage**: Comprehensive  
**Performance**: Production-ready for single-node workloads
